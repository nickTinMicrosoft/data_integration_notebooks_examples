{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Loading of the Raw Data into DL from Snowflake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import needed packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": null,
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "session_starting",
              "parent_msg_id": "ca1208d9-d7a5-46d7-aab0-9700d8869821",
              "queued_time": "2025-03-27T18:00:17.1757949Z",
              "session_id": null,
              "session_start_time": "2025-03-27T18:00:17.1766812Z",
              "spark_jobs": null,
              "spark_pool": null,
              "state": "session_starting",
              "statement_id": -1,
              "statement_ids": []
            },
            "text/plain": [
              "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, date, time\n",
        "import datetime\n",
        "import pytz\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the BUild Lists notebook to get our Small and Large Table Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run Variable_notebooks/Snowflake_BuildLists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This code below is to attach to an Azure Storage Account\n",
        "\n",
        "### This will need to be modified if writing to a Fabric One Lake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-28T15:26:37.7334101Z",
              "execution_start_time": "2025-02-28T15:26:37.5825156Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "7c640b5e-2795-46de-a255-bf367151ca4b",
              "queued_time": "2025-02-28T15:26:37.5091409Z",
              "session_id": "71",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DEVPoolNick",
              "state": "finished",
              "statement_id": 19,
              "statement_ids": [
                19
              ]
            },
            "text/plain": [
              "StatementMeta(DEVPoolNick, 71, 19, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "project_name = \"PROJECT NAME\"\n",
        "storage_account = \"AZURE STORAGE ACCOUNT NAME\"\n",
        "container_name = \"AZURE CONTAINER NAME\"\n",
        " \n",
        "\n",
        "##adding paths for Data Lake in storage account\n",
        "dl_path_bronze =\"BRONZE PATH\"\n",
        "dl_path_silver = \"SILVER PATH\"\n",
        "\n",
        "\n",
        "## This is to save data into a spark table, remove this line if using for Fabric One Lake\n",
        "spark.conf.set(\"myapp.dbname\",dbname)\n",
        "## ------------------\n",
        "\n",
        "## Build variables for the full paths to the folders in the Data Lake\n",
        "bronze_file_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{dl_path_bronze}/{project_name}\"\n",
        "silver_file_path = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{dl_path_silver}/{project_name}\"\n",
        "\n",
        "# Get the current date and time in EST\n",
        "now_est = datetime.datetime.now(pytz.timezone('US/Eastern'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(bronze_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(small_tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterate through the small_tables list and write data to .csv file\n",
        "\n",
        "\n",
        "**The files can be written to Parquet just by changing the .to_csv --> .to_parquet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for table in small_tables:\n",
        "    try:\n",
        "        print(f\"Selecting from {table}\")\n",
        "        query = f'SELECT * FROM \"{table}\"'\n",
        "        cursor.execute(query)\n",
        "    \n",
        "        rows = cursor.fetchall()\n",
        "        columns = [desc[0] for desc in cursor.description]\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "        file_name = f\"{bronze_file_path}/{table}.csv\"\n",
        "\n",
        "        df.to_csv(file_name,index=False)\n",
        "        \n",
        "        ## ----------------------------------\n",
        "        ## To Parquet Example\n",
        "        ##df.to_parquet(file_name.replace('.csv', '.parquet'), index=False) \n",
        "        ## ----------------------------------\n",
        "        \n",
        "        print(f\"Done with {table}\")\n",
        "        # cursor.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    except requests.exceptions.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.exceptions.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.exceptions.RequestException as err:\n",
        "        print(f\"Something went wrong: {err}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Writing to Microsoft Fabric OneLake\n",
        "\n",
        "This notebook shows a portable approach to write the DataFrame output into Microsoft Fabric OneLake by using the Microsoft Graph Drive API (works from outside Fabric notebooks). It provides:\n",
        "\n",
        "- A short explanation and assumptions.\n",
        "- Helper functions to convert DataFrames to bytes (CSV / Parquet).\n",
        "- A small-file upload (simple PUT) and a chunked upload using an upload session (for files > 4 MB).\n",
        "\n",
        "Assumptions:\n",
        "- You have an Azure AD app registration with client credentials (tenant_id, client_id, client_secret).\n",
        "- You know the OneLake drive id that represents the OneLake area where you want to write (we call it `oneLake_drive_id`).\n",
        "\n",
        "Notes:\n",
        "- Replace the placeholder values below with your real credentials and drive id.\n",
        "- For parquet support you will need `pyarrow` or `fastparquet` installed. For authentication we use `msal`.\n",
        "- This example writes CSV/parquet bytes to OneLake via Microsoft Graph. If you're running inside a Fabric notebook there may be simpler environment-native APIs; tell me if you want that instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### THIS CODE WAS BUILT BY COPILOT AND HAS NOT BEEN USED IN DEV OR TESTING USE AS EXAMPLE ONLY\n",
        "\n",
        "# OneLake upload helper functions (using Microsoft Graph)\n",
        "# Install msal if you don't have it: pip install msal\n",
        "import io\n",
        "import msal\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# --- Configuration (replace with your values) ---\n",
        "tenant_id = os.environ.get('AZURE_TENANT_ID', '<YOUR_TENANT_ID>')\n",
        "client_id = os.environ.get('AZURE_CLIENT_ID', '<YOUR_CLIENT_ID>')\n",
        "client_secret = os.environ.get('AZURE_CLIENT_SECRET', '<YOUR_CLIENT_SECRET>')\n",
        "oneLake_drive_id = os.environ.get('ONELAKE_DRIVE_ID', '<YOUR_ONELAKE_DRIVE_ID>')\n",
        "\n",
        "# Scopes for app-only Graph access\n",
        "scope = [\"https://graph.microsoft.com/.default\"]\n",
        "\n",
        "# Acquire token (client credentials)\n",
        "app = msal.ConfidentialClientApplication(client_id, authority=f\"https://login.microsoftonline.com/{tenant_id}\", client_credential=client_secret)\n",
        "resp = app.acquire_token_for_client(scopes=scope)\n",
        "if 'access_token' not in resp:\n",
        "    raise Exception(f\"Could not obtain access token: {resp}\")\n",
        "\n",
        "ACCESS_TOKEN = resp['access_token']\n",
        "HEADERS = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n",
        "\n",
        "\n",
        "def df_to_bytes(df, fmt='csv'):\n",
        "    \"\"\"Convert DataFrame to bytes in CSV or parquet format.\"\"\"\n",
        "    if fmt == 'csv':\n",
        "        bio = io.BytesIO()\n",
        "        df.to_csv(bio, index=False)\n",
        "        bio.seek(0)\n",
        "        return bio.read()\n",
        "    elif fmt == 'parquet':\n",
        "        bio = io.BytesIO()\n",
        "        df.to_parquet(bio, index=False)\n",
        "        bio.seek(0)\n",
        "        return bio.read()\n",
        "    else:\n",
        "        raise ValueError('Unsupported format')\n",
        "\n",
        "\n",
        "def upload_small_file(drive_id, path_in_drive, data_bytes, content_type='text/csv'):\n",
        "    \"\"\"Upload small files (<4MB) by simple PUT to the Graph item endpoint.\n",
        "    path_in_drive should be like '/path/to/file.csv' relative to the drive root.\n",
        "    \"\"\"\n",
        "    url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:{path_in_drive}:/content\"\n",
        "    headers = HEADERS.copy()\n",
        "    headers['Content-Type'] = content_type\n",
        "    r = requests.put(url, headers=headers, data=data_bytes)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "\n",
        "def create_upload_session(drive_id, path_in_drive):\n",
        "    url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:{path_in_drive}:/createUploadSession\"\n",
        "    r = requests.post(url, headers=HEADERS, json={\"item\": {\"@microsoft.graph.conflictBehavior\": \"replace\"}})\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "\n",
        "def upload_large_file_via_session(upload_url, data_bytes, chunk_size=5 * 1024 * 1024):\n",
        "    \"\"\"Upload bytes via an upload session. chunk_size defaults to 5MB (Graph minimum is 5MB for chunked uploads).\"\"\"\n",
        "    total = len(data_bytes)\n",
        "    start = 0\n",
        "    while start < total:\n",
        "        end = min(start + chunk_size, total) - 1\n",
        "        chunk = data_bytes[start:end+1]\n",
        "        headers = {\n",
        "            'Content-Length': str(len(chunk)),\n",
        "            'Content-Range': f'bytes {start}-{end}/{total}'\n",
        "        }\n",
        "        r = requests.put(upload_url, headers=headers, data=chunk)\n",
        "        if not (200 <= r.status_code < 300 or r.status_code == 201):\n",
        "            raise Exception(f\"Upload failed: {r.status_code} {r.text}\")\n",
        "        start = end + 1\n",
        "    return r.json()\n",
        "\n",
        "# Example usage inside the loop for small_tables (replace call to df.to_csv with upload_small_file)\n",
        "# For example in the earlier loop after df creation you can do:\n",
        "# bytes_data = df_to_bytes(df, fmt='csv')\n",
        "# remote_path = f\"/Bronze/{project_name}/{table}.csv\"\n",
        "# upload_small_file(oneLake_drive_id, remote_path, bytes_data, content_type='text/csv')\n",
        "\n",
        "print('OneLake helper functions loaded. Replace placeholders and call the upload functions where appropriate.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Iterate through large_tables and write data to .csv file\n",
        "\n",
        "\n",
        "***Can be written to Parquet***\n",
        "\n",
        "***This Code performs \"Chunking\" that was needed for a specific project. If you do not need this then just write contents to .csv or .parquet and remove the chunking code***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for table in largeTables:\n",
        "    import pandas as pd\n",
        "\n",
        "    #step 1 get id col from control table\n",
        "    #step 2 Look at table if id col exist if not drop from the table list and do not ingest \n",
        "    #step 3 ingest tables that match into dw\n",
        "\n",
        "    # Define chunk size\n",
        "    chunk_size = 400000\n",
        "\n",
        "    # Get total record count from Snowflake\n",
        "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
        "    record_count = cursor.fetchone()[0]\n",
        "\n",
        "    # Create chunks for processing\n",
        "    first_last_ids = [(first_id, last_id) for first_id, last_id in zip(range(1, record_count+1, chunk_size), range(chunk_size, record_count+1, chunk_size))]\n",
        "\n",
        "    for i, (first_id, last_id) in enumerate(first_last_ids, start=1):\n",
        "        print(f'Chunk {i}: First ID = {first_id}, Last ID = {last_id}')\n",
        "    \n",
        "        # Query to fetch the chunk of data\n",
        "        sql_query = f\"SELECT * FROM \"{table}\" WHERE Id BETWEEN {first_id} AND {last_id}\"\n",
        "        print(sql_query)\n",
        "\n",
        "        # Execute the query\n",
        "        cursor.execute(sql_query)\n",
        "    \n",
        "        # Fetch data and column names\n",
        "        rows = cursor.fetchall()\n",
        "        columns = [desc[0] for desc in cursor.description]\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(rows, columns=columns)\n",
        "    \n",
        "        # Save to CSV\n",
        "        filename = f'{bronze_file_path}/{table_name}-{i:02}.csv'\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f'Data from {table_name} saved in {filename}')\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Close the connector from the build_lists Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modifications for Fabric to consider\n",
        "\n",
        "- You may not want to break out the connection between notebooks like we did, you can put the cursor code into this one notebook and execute directly. By doing multiple notebooks you can make your conections to Snowflake More Dynamic\n",
        "- Code to write files will need to be modified so that you can write to Fabric Onelake. CODE ABOVE CREATED BY AI FOR EXAMPLE USE ONLY\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
